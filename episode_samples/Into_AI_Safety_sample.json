{
  "post_id": "81c52538-5af2-40aa-abd7-02c08088f43e",
  "title": "INTERVIEW: Scaling Democracy w/ (Dr.) Igor Krawczuk",
  "link": "https://kairos.fm/intoaisafety/e019",
  "published_parsed": "2024-06-03T16:11:00+00:00",
  "html_body": "<p>The <em>almost</em> Dr. Igor Krawczuk joins me for what is the equivalent of 4 of my previous episodes. We get into all the classics: eugenics, capitalism, philosophical toads... Need I say more?</p><p>If you're interested in connecting with Igor, head on over to his <a href=\"https://krawczuk.eu/\">website</a>, or check out <a href=\"https://github.com/into-ai-safety/into-ai-safety.github.io/blob/master/_posts\">placeholder for thesis</a> (it isn't published yet).</p><p>Because the full show notes have a whopping 115 additional links, I'll highlight some that I think are particularly worthwhile here:</p><ul><li>The best article you'll ever read on <a href=\"https://jacob-haimes.github.io/independent/Open-Source-AI-is-a-lie/\">Open Source AI</a></li><li>The best article you'll ever read on <a href=\"https://www.odysseaninstitute.org/post/let-s-talk-about-emergence\">emergence in ML</a></li><li>Kate Crawford's <a href=\"https://yalebooks.yale.edu/book/9780300264630/atlas-of-ai/\"><em>Atlas of AI</em></a> (<a href=\"https://en.wikipedia.org/wiki/Atlas_of_AI\">Wikipedia</a>)</li><li><a href=\"https://arxiv.org/abs/1911.01547\">On the Measure of Intelligence</a></li><li>Thomas Piketty's <a href=\"https://www.hup.harvard.edu/books/9780674430006\"><em>Capital in the Twenty-First Century</em></a> (<a href=\"https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century\">Wikipedia</a>)</li><li>Yurii Nesterov's <a href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=2-ElBQAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=info:JTiRBrZ_LZMJ:scholar.google.com&amp;ots=wnpRdsxjjv&amp;sig=1Oa-5P-zZZ_MX_2MFKv5cq2fx48#v=onepage&amp;q&amp;f=false\"><em>Introductory Lectures on Convex Optimization</em></a></li></ul><p><strong>Chapters<br /></strong></p><ul><li>(02:32) - Introducing Igor\n</li>\n<li>(10:11) - Aside on EY, LW, EA, etc., a.k.a. lettersoup\n</li>\n<li>(18:30) - Igor on AI alignment\n</li>\n<li>(33:06) - \"Open Source\" in AI\n</li>\n<li>(41:20) - The story of infinite riches and suffering\n</li>\n<li>(59:11) - On AI threat models\n</li>\n<li>(01:09:25) - Representation in AI\n</li>\n<li>(01:15:00) - Hazard fishing\n</li>\n<li>(01:18:52) - Intelligence and eugenics\n</li>\n<li>(01:34:38) - Emergence\n</li>\n<li>(01:48:19) - Considering externalities\n</li>\n<li>(01:53:33) - The shape of an argument\n</li>\n<li>(02:01:39) - More eugenics\n</li>\n<li>(02:06:09) - I'm convinced, what now?\n</li>\n<li>(02:18:03) - AIxBio (round ??)\n</li>\n<li>(02:29:09) - On open release of models\n</li>\n<li>(02:40:28) - Data and copyright\n</li>\n<li>(02:44:09) - Scientific accessibility and bullshit\n</li>\n<li>(02:53:04) - Igor's point of view\n</li>\n<li>(02:57:20) - Outro</li>\n</ul><p><strong><br />Links</strong></p><p>Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance. All references, including those only mentioned in the extended version of this episode, are included.</p><ul><li><a href=\"https://www.lighthousereports.com/suspicion-machines-methodology/\">Suspicious Machines Methodology</a>, referred to as the \"Rotterdam Lighthouse Report\" in the episode</li><li><a href=\"https://www.epfl.ch/labs/lions/\">LIONS Lab</a> at EPFL</li><li>The <a href=\"https://pbs.twimg.com/media/D53Q_MYW4AA-wRK.jpg\">meme</a> that Igor references</li><li><a href=\"https://arxiv.org/abs/2401.01869\">On the Hardness of Learning Under Symmetries</a></li><li><a href=\"https://uvagedl.github.io/\">Course</a> on the concept of equivariant deep learning</li><li>Aside on EY/EA/etc.<ul><li>Sources on Eliezer Yudkowski<ul><li><a href=\"https://encyclopedia.pub/entry/33978\">Scholarly Community Encyclopedia</a></li><li><a href=\"https://time.com/collection/time100-ai/6309037/eliezer-yudkowsky/\">TIME100 AI</a></li><li>Yudkowski's personal <a href=\"https://www.yudkowsky.net/\">website</a></li><li><a href=\"https://en.wikipedia.org/wiki/Eliezer_Yudkowsky\">EY Wikipedia</a></li><li><a href=\"https://whatshouldiread.fandom.com/wiki/Eliezer_Yudkowsky#cite_note-1\">A Very Literary Wiki</a> -TIME article: <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Pausing AI Developments Isnâ€™t Enough. We Need to Shut it All Down</a> documenting EY's ruminations of bombing datacenters; this comes up later in the episode but is included here because it about EY.</li></ul></li><li><a href=\"https://www.lesswrong.com/\">LessWrong</a><ul><li><a href=\"https://en.wikipedia.org/wiki/LessWrong\">LW Wikipedia</a></li></ul></li><li><a href=\"https://intelligence.org/\">MIRI</a></li><li>Coverage on Nick Bostrom (being a racist)<ul><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/28/nick-bostrom-controversial-future-of-humanity-institute-closure-longtermism-affective-altruism\">â€˜Eugenics on steroidsâ€™: the toxic and contested legacy of Oxfordâ€™s Future of Humanity Institute</a></li><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/19/oxford-future-of-humanity-institute-closes\">Oxford shuts down institute run by Elon Musk-backed philosopher</a></li></ul></li><li>Investigative <a href=\"https://markfuentes1.substack.com/p/emile-p-torress-history-of-dishonesty\">piece</a> on Ã‰mile Torres</li><li><a href=\"https://dl.acm.org/doi/10.1145/3442188.3445922\">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2019/11/11/technology/artificial-intelligence-bias.html\">We Teach A.I. Systems Everything, Including Our Biases</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html\">Google Researcher Says She Was Fired Over Paper Highlighting Bias in A.I.</a></li><li>Timnit Gebru's <a href=\"https://en.wikipedia.org/wiki/Timnit_Gebru\">Wikipedia</a></li><li><a href=\"https://firstmonday.org/ojs/index.php/fm/article/view/13636\">The TESCREAL Bundle: Eugenics and the Promise of Utopia through Artificial General Intelligence</a></li><li>Sources on the environmental impact of LLMs<ul><li><a href=\"https://analyticsindiamag.com/the-environmental-impact-of-llms/\">The Environmental Impact of LLMs</a></li><li><a href=\"https://tinyml.substack.com/p/the-cost-of-inference-running-the\">The Cost of Inference: Running the Models</a></li><li><a href=\"https://arxiv.org/abs/1906.02243\">Energy and Policy Considerations for Deep Learning in NLP</a></li><li><a href=\"https://weareyard.com/insights/the-carbon-impact-of-ai-vs-search-engines\">The Carbon Impact of AI vs Search Engines</a></li></ul></li></ul></li><li><a href=\"https://www.science.org/doi/full/10.1126/science.abi7176?casa_token=2txe0r_jjhQAAAAA%3ALJa__HZL9COyj9EUpdILZdtnMKLyggfFe7Zpvv0tNze62rLO0CoQHCCJiXfruxUeBLj3YBZ33F8OOv0u\">Filling Gaps in Trustworthy Development of AI </a>(Igor is an author on this one)</li><li><a href=\"https://www.hindawi.com/journals/complexity/2022/8210732/\">A Computational Turn in Policy Process Studies: Coevolving Network Dynamics of Policy Change</a></li><li><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/file/7e05d6f828574fbc975a896b25bb011e-Paper.pdf\">The Smoothed Possibility of Social Choice</a>, an intro in social choice theory and how it overlaps with ML</li><li>Relating to Dan Hendrycks<ul><li><a href=\"https://arxiv.org/abs/2303.16200\">Natural Selection Favors AIs over Humans</a><ul><li>\"One easy-to-digest source to highlight what he gets wrong [is] <a href=\"https://pressbooks.calstate.edu/explorationsbioanth2/chapter/17/\">Social and Biopolitical Dimensions of Evolutionary Thinking</a>\" -Igor</li></ul></li><li><a href=\"https://www.aisafetybook.com/\">Introduction to AI Safety, Ethics, and Society</a>, recently published textbook</li><li>\"<a href=\"https://arxiv.org/pdf/2306.12001#page=10.19\">Source</a> to the section [of this paper] that makes Dan one of my favs from that crowd.\" -Igor</li><li><a href=\"https://twitter.com/DanHendrycks/status/1710312043503321141\">Twitter post</a> referenced in the episode&lt;...</li></ul></li></ul>",
  "audio_url": "https://op3.dev/e/media.transistor.fm/b8225038/75bd8800.mp3",
  "image_url": null,
  "authors_list": [
    "Jacob Haimes"
  ],
  "tags": [
    "AI",
    "machine learning",
    "democracy",
    "AI safety"
  ],
  "source_name": "Into AI Safety",
  "raw_entry": {
    "title": "INTERVIEW: Scaling Democracy w/ (Dr.) Igor Krawczuk",
    "title_detail": {
      "type": "text/plain",
      "language": null,
      "base": "https://feeds.transistor.fm/intoaisafety",
      "value": "INTERVIEW: Scaling Democracy w/ (Dr.) Igor Krawczuk"
    },
    "itunes_episode": "19",
    "podcast_episode": "19",
    "itunes_title": "INTERVIEW: Scaling Democracy w/ (Dr.) Igor Krawczuk",
    "itunes_episodetype": "full",
    "id": "81c52538-5af2-40aa-abd7-02c08088f43e",
    "guidislink": false,
    "links": [
      {
        "rel": "alternate",
        "type": "text/html",
        "href": "https://kairos.fm/intoaisafety/e019"
      },
      {
        "length": "171643841",
        "type": "audio/mpeg",
        "href": "https://op3.dev/e/media.transistor.fm/b8225038/75bd8800.mp3",
        "rel": "enclosure"
      }
    ],
    "link": "https://kairos.fm/intoaisafety/e019",
    "summary": "<p>The <em>almost</em> Dr. Igor Krawczuk joins me for what is the equivalent of 4 of my previous episodes. We get into all the classics: eugenics, capitalism, philosophical toads... Need I say more?</p><p>If you're interested in connecting with Igor, head on over to his <a href=\"https://krawczuk.eu/\">website</a>, or check out <a href=\"https://github.com/into-ai-safety/into-ai-safety.github.io/blob/master/_posts\">placeholder for thesis</a> (it isn't published yet).</p><p>Because the full show notes have a whopping 115 additional links, I'll highlight some that I think are particularly worthwhile here:</p><ul><li>The best article you'll ever read on <a href=\"https://jacob-haimes.github.io/independent/Open-Source-AI-is-a-lie/\">Open Source AI</a></li><li>The best article you'll ever read on <a href=\"https://www.odysseaninstitute.org/post/let-s-talk-about-emergence\">emergence in ML</a></li><li>Kate Crawford's <a href=\"https://yalebooks.yale.edu/book/9780300264630/atlas-of-ai/\"><em>Atlas of AI</em></a> (<a href=\"https://en.wikipedia.org/wiki/Atlas_of_AI\">Wikipedia</a>)</li><li><a href=\"https://arxiv.org/abs/1911.01547\">On the Measure of Intelligence</a></li><li>Thomas Piketty's <a href=\"https://www.hup.harvard.edu/books/9780674430006\"><em>Capital in the Twenty-First Century</em></a> (<a href=\"https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century\">Wikipedia</a>)</li><li>Yurii Nesterov's <a href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=2-ElBQAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=info:JTiRBrZ_LZMJ:scholar.google.com&amp;ots=wnpRdsxjjv&amp;sig=1Oa-5P-zZZ_MX_2MFKv5cq2fx48#v=onepage&amp;q&amp;f=false\"><em>Introductory Lectures on Convex Optimization</em></a></li></ul><p><strong>Chapters<br /></strong></p><ul><li>(02:32) - Introducing Igor\n</li>\n<li>(10:11) - Aside on EY, LW, EA, etc., a.k.a. lettersoup\n</li>\n<li>(18:30) - Igor on AI alignment\n</li>\n<li>(33:06) - \"Open Source\" in AI\n</li>\n<li>(41:20) - The story of infinite riches and suffering\n</li>\n<li>(59:11) - On AI threat models\n</li>\n<li>(01:09:25) - Representation in AI\n</li>\n<li>(01:15:00) - Hazard fishing\n</li>\n<li>(01:18:52) - Intelligence and eugenics\n</li>\n<li>(01:34:38) - Emergence\n</li>\n<li>(01:48:19) - Considering externalities\n</li>\n<li>(01:53:33) - The shape of an argument\n</li>\n<li>(02:01:39) - More eugenics\n</li>\n<li>(02:06:09) - I'm convinced, what now?\n</li>\n<li>(02:18:03) - AIxBio (round ??)\n</li>\n<li>(02:29:09) - On open release of models\n</li>\n<li>(02:40:28) - Data and copyright\n</li>\n<li>(02:44:09) - Scientific accessibility and bullshit\n</li>\n<li>(02:53:04) - Igor's point of view\n</li>\n<li>(02:57:20) - Outro</li>\n</ul><p><strong><br />Links</strong></p><p>Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance. All references, including those only mentioned in the extended version of this episode, are included.</p><ul><li><a href=\"https://www.lighthousereports.com/suspicion-machines-methodology/\">Suspicious Machines Methodology</a>, referred to as the \"Rotterdam Lighthouse Report\" in the episode</li><li><a href=\"https://www.epfl.ch/labs/lions/\">LIONS Lab</a> at EPFL</li><li>The <a href=\"https://pbs.twimg.com/media/D53Q_MYW4AA-wRK.jpg\">meme</a> that Igor references</li><li><a href=\"https://arxiv.org/abs/2401.01869\">On the Hardness of Learning Under Symmetries</a></li><li><a href=\"https://uvagedl.github.io/\">Course</a> on the concept of equivariant deep learning</li><li>Aside on EY/EA/etc.<ul><li>Sources on Eliezer Yudkowski<ul><li><a href=\"https://encyclopedia.pub/entry/33978\">Scholarly Community Encyclopedia</a></li><li><a href=\"https://time.com/collection/time100-ai/6309037/eliezer-yudkowsky/\">TIME100 AI</a></li><li>Yudkowski's personal <a href=\"https://www.yudkowsky.net/\">website</a></li><li><a href=\"https://en.wikipedia.org/wiki/Eliezer_Yudkowsky\">EY Wikipedia</a></li><li><a href=\"https://whatshouldiread.fandom.com/wiki/Eliezer_Yudkowsky#cite_note-1\">A Very Literary Wiki</a> -TIME article: <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Pausing AI Developments Isnâ€™t Enough. We Need to Shut it All Down</a> documenting EY's ruminations of bombing datacenters; this comes up later in the episode but is included here because it about EY.</li></ul></li><li><a href=\"https://www.lesswrong.com/\">LessWrong</a><ul><li><a href=\"https://en.wikipedia.org/wiki/LessWrong\">LW Wikipedia</a></li></ul></li><li><a href=\"https://intelligence.org/\">MIRI</a></li><li>Coverage on Nick Bostrom (being a racist)<ul><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/28/nick-bostrom-controversial-future-of-humanity-institute-closure-longtermism-affective-altruism\">â€˜Eugenics on steroidsâ€™: the toxic and contested legacy of Oxfordâ€™s Future of Humanity Institute</a></li><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/19/oxford-future-of-humanity-institute-closes\">Oxford shuts down institute run by Elon Musk-backed philosopher</a></li></ul></li><li>Investigative <a href=\"https://markfuentes1.substack.com/p/emile-p-torress-history-of-dishonesty\">piece</a> on Ã‰mile Torres</li><li><a href=\"https://dl.acm.org/doi/10.1145/3442188.3445922\">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2019/11/11/technology/artificial-intelligence-bias.html\">We Teach A.I. Systems Everything, Including Our Biases</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html\">Google Researcher Says She Was Fired Over Paper Highlighting Bias in A.I.</a></li><li>Timnit Gebru's <a href=\"https://en.wikipedia.org/wiki/Timnit_Gebru\">Wikipedia</a></li><li><a href=\"https://firstmonday.org/ojs/index.php/fm/article/view/13636\">The TESCREAL Bundle: Eugenics and the Promise of Utopia through Artificial General Intelligence</a></li><li>Sources on the environmental impact of LLMs<ul><li><a href=\"https://analyticsindiamag.com/the-environmental-impact-of-llms/\">The Environmental Impact of LLMs</a></li><li><a href=\"https://tinyml.substack.com/p/the-cost-of-inference-running-the\">The Cost of Inference: Running the Models</a></li><li><a href=\"https://arxiv.org/abs/1906.02243\">Energy and Policy Considerations for Deep Learning in NLP</a></li><li><a href=\"https://weareyard.com/insights/the-carbon-impact-of-ai-vs-search-engines\">The Carbon Impact of AI vs Search Engines</a></li></ul></li></ul></li><li><a href=\"https://www.science.org/doi/full/10.1126/science.abi7176?casa_token=2txe0r_jjhQAAAAA%3ALJa__HZL9COyj9EUpdILZdtnMKLyggfFe7Zpvv0tNze62rLO0CoQHCCJiXfruxUeBLj3YBZ33F8OOv0u\">Filling Gaps in Trustworthy Development of AI </a>(Igor is an author on this one)</li><li><a href=\"https://www.hindawi.com/journals/complexity/2022/8210732/\">A Computational Turn in Policy Process Studies: Coevolving Network Dynamics of Policy Change</a></li><li><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/file/7e05d6f828574fbc975a896b25bb011e-Paper.pdf\">The Smoothed Possibility of Social Choice</a>, an intro in social choice theory and how it overlaps with ML</li><li>Relating to Dan Hendrycks<ul><li><a href=\"https://arxiv.org/abs/2303.16200\">Natural Selection Favors AIs over Humans</a><ul><li>\"One easy-to-digest source to highlight what he gets wrong [is] <a href=\"https://pressbooks.calstate.edu/explorationsbioanth2/chapter/17/\">Social and Biopolitical Dimensions of Evolutionary Thinking</a>\" -Igor</li></ul></li><li><a href=\"https://www.aisafetybook.com/\">Introduction to AI Safety, Ethics, and Society</a>, recently published textbook</li><li>\"<a href=\"https://arxiv.org/pdf/2306.12001#page=10.19\">Source</a> to the section [of this paper] that makes Dan one of my favs from that crowd.\" -Igor</li><li><a href=\"https://twitter.com/DanHendrycks/status/1710312043503321141\">Twitter post</a> referenced in the episode&lt;...</li></ul></li></ul>",
    "summary_detail": {
      "type": "text/html",
      "language": null,
      "base": "https://feeds.transistor.fm/intoaisafety",
      "value": "<p>The <em>almost</em> Dr. Igor Krawczuk joins me for what is the equivalent of 4 of my previous episodes. We get into all the classics: eugenics, capitalism, philosophical toads... Need I say more?</p><p>If you're interested in connecting with Igor, head on over to his <a href=\"https://krawczuk.eu/\">website</a>, or check out <a href=\"https://github.com/into-ai-safety/into-ai-safety.github.io/blob/master/_posts\">placeholder for thesis</a> (it isn't published yet).</p><p>Because the full show notes have a whopping 115 additional links, I'll highlight some that I think are particularly worthwhile here:</p><ul><li>The best article you'll ever read on <a href=\"https://jacob-haimes.github.io/independent/Open-Source-AI-is-a-lie/\">Open Source AI</a></li><li>The best article you'll ever read on <a href=\"https://www.odysseaninstitute.org/post/let-s-talk-about-emergence\">emergence in ML</a></li><li>Kate Crawford's <a href=\"https://yalebooks.yale.edu/book/9780300264630/atlas-of-ai/\"><em>Atlas of AI</em></a> (<a href=\"https://en.wikipedia.org/wiki/Atlas_of_AI\">Wikipedia</a>)</li><li><a href=\"https://arxiv.org/abs/1911.01547\">On the Measure of Intelligence</a></li><li>Thomas Piketty's <a href=\"https://www.hup.harvard.edu/books/9780674430006\"><em>Capital in the Twenty-First Century</em></a> (<a href=\"https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century\">Wikipedia</a>)</li><li>Yurii Nesterov's <a href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=2-ElBQAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=info:JTiRBrZ_LZMJ:scholar.google.com&amp;ots=wnpRdsxjjv&amp;sig=1Oa-5P-zZZ_MX_2MFKv5cq2fx48#v=onepage&amp;q&amp;f=false\"><em>Introductory Lectures on Convex Optimization</em></a></li></ul><p><strong>Chapters<br /></strong></p><ul><li>(02:32) - Introducing Igor\n</li>\n<li>(10:11) - Aside on EY, LW, EA, etc., a.k.a. lettersoup\n</li>\n<li>(18:30) - Igor on AI alignment\n</li>\n<li>(33:06) - \"Open Source\" in AI\n</li>\n<li>(41:20) - The story of infinite riches and suffering\n</li>\n<li>(59:11) - On AI threat models\n</li>\n<li>(01:09:25) - Representation in AI\n</li>\n<li>(01:15:00) - Hazard fishing\n</li>\n<li>(01:18:52) - Intelligence and eugenics\n</li>\n<li>(01:34:38) - Emergence\n</li>\n<li>(01:48:19) - Considering externalities\n</li>\n<li>(01:53:33) - The shape of an argument\n</li>\n<li>(02:01:39) - More eugenics\n</li>\n<li>(02:06:09) - I'm convinced, what now?\n</li>\n<li>(02:18:03) - AIxBio (round ??)\n</li>\n<li>(02:29:09) - On open release of models\n</li>\n<li>(02:40:28) - Data and copyright\n</li>\n<li>(02:44:09) - Scientific accessibility and bullshit\n</li>\n<li>(02:53:04) - Igor's point of view\n</li>\n<li>(02:57:20) - Outro</li>\n</ul><p><strong><br />Links</strong></p><p>Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance. All references, including those only mentioned in the extended version of this episode, are included.</p><ul><li><a href=\"https://www.lighthousereports.com/suspicion-machines-methodology/\">Suspicious Machines Methodology</a>, referred to as the \"Rotterdam Lighthouse Report\" in the episode</li><li><a href=\"https://www.epfl.ch/labs/lions/\">LIONS Lab</a> at EPFL</li><li>The <a href=\"https://pbs.twimg.com/media/D53Q_MYW4AA-wRK.jpg\">meme</a> that Igor references</li><li><a href=\"https://arxiv.org/abs/2401.01869\">On the Hardness of Learning Under Symmetries</a></li><li><a href=\"https://uvagedl.github.io/\">Course</a> on the concept of equivariant deep learning</li><li>Aside on EY/EA/etc.<ul><li>Sources on Eliezer Yudkowski<ul><li><a href=\"https://encyclopedia.pub/entry/33978\">Scholarly Community Encyclopedia</a></li><li><a href=\"https://time.com/collection/time100-ai/6309037/eliezer-yudkowsky/\">TIME100 AI</a></li><li>Yudkowski's personal <a href=\"https://www.yudkowsky.net/\">website</a></li><li><a href=\"https://en.wikipedia.org/wiki/Eliezer_Yudkowsky\">EY Wikipedia</a></li><li><a href=\"https://whatshouldiread.fandom.com/wiki/Eliezer_Yudkowsky#cite_note-1\">A Very Literary Wiki</a> -TIME article: <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Pausing AI Developments Isnâ€™t Enough. We Need to Shut it All Down</a> documenting EY's ruminations of bombing datacenters; this comes up later in the episode but is included here because it about EY.</li></ul></li><li><a href=\"https://www.lesswrong.com/\">LessWrong</a><ul><li><a href=\"https://en.wikipedia.org/wiki/LessWrong\">LW Wikipedia</a></li></ul></li><li><a href=\"https://intelligence.org/\">MIRI</a></li><li>Coverage on Nick Bostrom (being a racist)<ul><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/28/nick-bostrom-controversial-future-of-humanity-institute-closure-longtermism-affective-altruism\">â€˜Eugenics on steroidsâ€™: the toxic and contested legacy of Oxfordâ€™s Future of Humanity Institute</a></li><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/19/oxford-future-of-humanity-institute-closes\">Oxford shuts down institute run by Elon Musk-backed philosopher</a></li></ul></li><li>Investigative <a href=\"https://markfuentes1.substack.com/p/emile-p-torress-history-of-dishonesty\">piece</a> on Ã‰mile Torres</li><li><a href=\"https://dl.acm.org/doi/10.1145/3442188.3445922\">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2019/11/11/technology/artificial-intelligence-bias.html\">We Teach A.I. Systems Everything, Including Our Biases</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html\">Google Researcher Says She Was Fired Over Paper Highlighting Bias in A.I.</a></li><li>Timnit Gebru's <a href=\"https://en.wikipedia.org/wiki/Timnit_Gebru\">Wikipedia</a></li><li><a href=\"https://firstmonday.org/ojs/index.php/fm/article/view/13636\">The TESCREAL Bundle: Eugenics and the Promise of Utopia through Artificial General Intelligence</a></li><li>Sources on the environmental impact of LLMs<ul><li><a href=\"https://analyticsindiamag.com/the-environmental-impact-of-llms/\">The Environmental Impact of LLMs</a></li><li><a href=\"https://tinyml.substack.com/p/the-cost-of-inference-running-the\">The Cost of Inference: Running the Models</a></li><li><a href=\"https://arxiv.org/abs/1906.02243\">Energy and Policy Considerations for Deep Learning in NLP</a></li><li><a href=\"https://weareyard.com/insights/the-carbon-impact-of-ai-vs-search-engines\">The Carbon Impact of AI vs Search Engines</a></li></ul></li></ul></li><li><a href=\"https://www.science.org/doi/full/10.1126/science.abi7176?casa_token=2txe0r_jjhQAAAAA%3ALJa__HZL9COyj9EUpdILZdtnMKLyggfFe7Zpvv0tNze62rLO0CoQHCCJiXfruxUeBLj3YBZ33F8OOv0u\">Filling Gaps in Trustworthy Development of AI </a>(Igor is an author on this one)</li><li><a href=\"https://www.hindawi.com/journals/complexity/2022/8210732/\">A Computational Turn in Policy Process Studies: Coevolving Network Dynamics of Policy Change</a></li><li><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/file/7e05d6f828574fbc975a896b25bb011e-Paper.pdf\">The Smoothed Possibility of Social Choice</a>, an intro in social choice theory and how it overlaps with ML</li><li>Relating to Dan Hendrycks<ul><li><a href=\"https://arxiv.org/abs/2303.16200\">Natural Selection Favors AIs over Humans</a><ul><li>\"One easy-to-digest source to highlight what he gets wrong [is] <a href=\"https://pressbooks.calstate.edu/explorationsbioanth2/chapter/17/\">Social and Biopolitical Dimensions of Evolutionary Thinking</a>\" -Igor</li></ul></li><li><a href=\"https://www.aisafetybook.com/\">Introduction to AI Safety, Ethics, and Society</a>, recently published textbook</li><li>\"<a href=\"https://arxiv.org/pdf/2306.12001#page=10.19\">Source</a> to the section [of this paper] that makes Dan one of my favs from that crowd.\" -Igor</li><li><a href=\"https://twitter.com/DanHendrycks/status/1710312043503321141\">Twitter post</a> referenced in the episode&lt;...</li></ul></li></ul>"
    },
    "content": [
      {
        "type": "text/html",
        "language": null,
        "base": "https://feeds.transistor.fm/intoaisafety",
        "value": "<p>The <em>almost</em> Dr. Igor Krawczuk joins me for what is the equivalent of 4 of my previous episodes. We get into all the classics: eugenics, capitalism, philosophical toads... Need I say more?</p><p>If you're interested in connecting with Igor, head on over to his <a href=\"https://krawczuk.eu/\">website</a>, or check out <a href=\"https://github.com/into-ai-safety/into-ai-safety.github.io/blob/master/_posts\">placeholder for thesis</a> (it isn't published yet).</p><p>Because the full show notes have a whopping 115 additional links, I'll highlight some that I think are particularly worthwhile here:</p><ul><li>The best article you'll ever read on <a href=\"https://jacob-haimes.github.io/independent/Open-Source-AI-is-a-lie/\">Open Source AI</a></li><li>The best article you'll ever read on <a href=\"https://www.odysseaninstitute.org/post/let-s-talk-about-emergence\">emergence in ML</a></li><li>Kate Crawford's <a href=\"https://yalebooks.yale.edu/book/9780300264630/atlas-of-ai/\"><em>Atlas of AI</em></a> (<a href=\"https://en.wikipedia.org/wiki/Atlas_of_AI\">Wikipedia</a>)</li><li><a href=\"https://arxiv.org/abs/1911.01547\">On the Measure of Intelligence</a></li><li>Thomas Piketty's <a href=\"https://www.hup.harvard.edu/books/9780674430006\"><em>Capital in the Twenty-First Century</em></a> (<a href=\"https://en.wikipedia.org/wiki/Capital_in_the_Twenty-First_Century\">Wikipedia</a>)</li><li>Yurii Nesterov's <a href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=2-ElBQAAQBAJ&amp;oi=fnd&amp;pg=PA1&amp;dq=info:JTiRBrZ_LZMJ:scholar.google.com&amp;ots=wnpRdsxjjv&amp;sig=1Oa-5P-zZZ_MX_2MFKv5cq2fx48#v=onepage&amp;q&amp;f=false\"><em>Introductory Lectures on Convex Optimization</em></a></li></ul><p><strong>Chapters<br /></strong></p><ul><li>(02:32) - Introducing Igor\n</li>\n<li>(10:11) - Aside on EY, LW, EA, etc., a.k.a. lettersoup\n</li>\n<li>(18:30) - Igor on AI alignment\n</li>\n<li>(33:06) - \"Open Source\" in AI\n</li>\n<li>(41:20) - The story of infinite riches and suffering\n</li>\n<li>(59:11) - On AI threat models\n</li>\n<li>(01:09:25) - Representation in AI\n</li>\n<li>(01:15:00) - Hazard fishing\n</li>\n<li>(01:18:52) - Intelligence and eugenics\n</li>\n<li>(01:34:38) - Emergence\n</li>\n<li>(01:48:19) - Considering externalities\n</li>\n<li>(01:53:33) - The shape of an argument\n</li>\n<li>(02:01:39) - More eugenics\n</li>\n<li>(02:06:09) - I'm convinced, what now?\n</li>\n<li>(02:18:03) - AIxBio (round ??)\n</li>\n<li>(02:29:09) - On open release of models\n</li>\n<li>(02:40:28) - Data and copyright\n</li>\n<li>(02:44:09) - Scientific accessibility and bullshit\n</li>\n<li>(02:53:04) - Igor's point of view\n</li>\n<li>(02:57:20) - Outro</li>\n</ul><p><strong><br />Links</strong></p><p>Links to all articles/papers which are mentioned throughout the episode can be found below, in order of their appearance. All references, including those only mentioned in the extended version of this episode, are included.</p><ul><li><a href=\"https://www.lighthousereports.com/suspicion-machines-methodology/\">Suspicious Machines Methodology</a>, referred to as the \"Rotterdam Lighthouse Report\" in the episode</li><li><a href=\"https://www.epfl.ch/labs/lions/\">LIONS Lab</a> at EPFL</li><li>The <a href=\"https://pbs.twimg.com/media/D53Q_MYW4AA-wRK.jpg\">meme</a> that Igor references</li><li><a href=\"https://arxiv.org/abs/2401.01869\">On the Hardness of Learning Under Symmetries</a></li><li><a href=\"https://uvagedl.github.io/\">Course</a> on the concept of equivariant deep learning</li><li>Aside on EY/EA/etc.<ul><li>Sources on Eliezer Yudkowski<ul><li><a href=\"https://encyclopedia.pub/entry/33978\">Scholarly Community Encyclopedia</a></li><li><a href=\"https://time.com/collection/time100-ai/6309037/eliezer-yudkowsky/\">TIME100 AI</a></li><li>Yudkowski's personal <a href=\"https://www.yudkowsky.net/\">website</a></li><li><a href=\"https://en.wikipedia.org/wiki/Eliezer_Yudkowsky\">EY Wikipedia</a></li><li><a href=\"https://whatshouldiread.fandom.com/wiki/Eliezer_Yudkowsky#cite_note-1\">A Very Literary Wiki</a> -TIME article: <a href=\"https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/\">Pausing AI Developments Isnâ€™t Enough. We Need to Shut it All Down</a> documenting EY's ruminations of bombing datacenters; this comes up later in the episode but is included here because it about EY.</li></ul></li><li><a href=\"https://www.lesswrong.com/\">LessWrong</a><ul><li><a href=\"https://en.wikipedia.org/wiki/LessWrong\">LW Wikipedia</a></li></ul></li><li><a href=\"https://intelligence.org/\">MIRI</a></li><li>Coverage on Nick Bostrom (being a racist)<ul><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/28/nick-bostrom-controversial-future-of-humanity-institute-closure-longtermism-affective-altruism\">â€˜Eugenics on steroidsâ€™: the toxic and contested legacy of Oxfordâ€™s Future of Humanity Institute</a></li><li>The Guardian article: <a href=\"https://www.theguardian.com/technology/2024/apr/19/oxford-future-of-humanity-institute-closes\">Oxford shuts down institute run by Elon Musk-backed philosopher</a></li></ul></li><li>Investigative <a href=\"https://markfuentes1.substack.com/p/emile-p-torress-history-of-dishonesty\">piece</a> on Ã‰mile Torres</li><li><a href=\"https://dl.acm.org/doi/10.1145/3442188.3445922\">On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2019/11/11/technology/artificial-intelligence-bias.html\">We Teach A.I. Systems Everything, Including Our Biases</a></li><li>NY Times article: <a href=\"https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html\">Google Researcher Says She Was Fired Over Paper Highlighting Bias in A.I.</a></li><li>Timnit Gebru's <a href=\"https://en.wikipedia.org/wiki/Timnit_Gebru\">Wikipedia</a></li><li><a href=\"https://firstmonday.org/ojs/index.php/fm/article/view/13636\">The TESCREAL Bundle: Eugenics and the Promise of Utopia through Artificial General Intelligence</a></li><li>Sources on the environmental impact of LLMs<ul><li><a href=\"https://analyticsindiamag.com/the-environmental-impact-of-llms/\">The Environmental Impact of LLMs</a></li><li><a href=\"https://tinyml.substack.com/p/the-cost-of-inference-running-the\">The Cost of Inference: Running the Models</a></li><li><a href=\"https://arxiv.org/abs/1906.02243\">Energy and Policy Considerations for Deep Learning in NLP</a></li><li><a href=\"https://weareyard.com/insights/the-carbon-impact-of-ai-vs-search-engines\">The Carbon Impact of AI vs Search Engines</a></li></ul></li></ul></li><li><a href=\"https://www.science.org/doi/full/10.1126/science.abi7176?casa_token=2txe0r_jjhQAAAAA%3ALJa__HZL9COyj9EUpdILZdtnMKLyggfFe7Zpvv0tNze62rLO0CoQHCCJiXfruxUeBLj3YBZ33F8OOv0u\">Filling Gaps in Trustworthy Development of AI </a>(Igor is an author on this one)</li><li><a href=\"https://www.hindawi.com/journals/complexity/2022/8210732/\">A Computational Turn in Policy Process Studies: Coevolving Network Dynamics of Policy Change</a></li><li><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/file/7e05d6f828574fbc975a896b25bb011e-Paper.pdf\">The Smoothed Possibility of Social Choice</a>, an intro in social choice theory and how it overlaps with ML</li><li>Relating to Dan Hendrycks<ul><li><a href=\"https://arxiv.org/abs/2303.16200\">Natural Selection Favors AIs over Humans</a><ul><li>\"One easy-to-digest source to highlight what he gets wrong [is] <a href=\"https://pressbooks.calstate.edu/explorationsbioanth2/chapter/17/\">Social and Biopolitical Dimensions of Evolutionary Thinking</a>\" -Igor</li></ul></li><li><a href=\"https://www.aisafetybook.com/\">Introduction to AI Safety, Ethics, and Society</a>, recently published textbook</li><li>\"<a href=\"https://arxiv.org/pdf/2306.12001#page=10.19\">Source</a> to the section [of this paper] that makes Dan one of my favs from that crowd.\" -Igor</li><li><a href=\"https://twitter.com/DanHendrycks/status/1710312043503321141\">Twitter post</a> referenced in the episode&lt;...</li></ul></li></ul>"
      }
    ],
    "published": "Mon, 03 Jun 2024 10:11:00 -0600",
    "published_parsed": [
      2024,
      6,
      3,
      16,
      11,
      0,
      0,
      155,
      0
    ],
    "authors": [
      {
        "name": "Jacob Haimes"
      },
      {
        "name": "Jacob Haimes"
      }
    ],
    "author": "Jacob Haimes",
    "author_detail": {
      "name": "Jacob Haimes"
    },
    "itunes_duration": "10726",
    "tags": [
      {
        "term": "AI",
        "scheme": "http://www.itunes.com/",
        "label": null
      },
      {
        "term": "machine learning",
        "scheme": "http://www.itunes.com/",
        "label": null
      },
      {
        "term": "democracy",
        "scheme": "http://www.itunes.com/",
        "label": null
      },
      {
        "term": "AI safety",
        "scheme": "http://www.itunes.com/",
        "label": null
      }
    ],
    "itunes_explicit": null,
    "podcast_person": {
      "role": "Guest",
      "href": "https://krawczuk.eu",
      "img": "https://img.transistor.fm/VJHrDED4yyNE9cAwg5Qkc13UxtWcUraWj_NfKbV2HIc/rs:fill:800:800:1/q:60/aHR0cHM6Ly9pbWct/dXBsb2FkLXByb2R1/Y3Rpb24udHJhbnNp/c3Rvci5mbS9jZTE4/YTI3ZTgyYjY4YzFh/OTY0NDkzNmZjZTUz/YjVkYi5qcGVn.jpg"
    },
    "podcast_chapters": {
      "url": "https://share.transistor.fm/s/b8225038/chapters.json",
      "type": "application/json+chapters"
    }
  }
}